{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J_ghSNft8rB"
   },
   "source": [
    "**Caso de Estudio**\n",
    "## Machine Learning Avanzado\n",
    "## Proyecto: Outbrain Click Prediction\n",
    "\n",
    "Integrantes: Carlos Bustamante, Nicolás Rivera, Pablo Elgueta y Patricio Ramirez.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E2tWz2NwTbh"
   },
   "source": [
    "El caso seleccionado es un desafío de Kaggle del año 2017, donde se busca predecir el contenido que una persona cliquearía en distintas páginas web. El desafío [Outbrain Click Prediction](https://www.kaggle.com/competitions/outbrain-click-prediction/overview) sorteó USD$25.000 en premios para los tres mejores lugares y contó con una amplia participación durante el concurso.\n",
    "\n",
    "El dataset contine muestras de vistas y clicks de usuarios en Estados Unidos observados durante el 2016. Cada página web o aviso publicitario clickeado (display_id) contiene además información de sus características. También hay información de recomendaciones dadas a usuarios específicos en contextos específicos.\n",
    "\n",
    "Para su resolución se seguiran las siguientes etapas:\n",
    "\n",
    "*   Reconocimiento e importación de las librerías y módulos utilizados\n",
    "*   Procesamiento de datos\n",
    "*   Exploración descriptiva \n",
    "*   Aplicación de un modelo de factorización de matrices\n",
    "*   Aplicación de un modelo de factorización de máquinas\n",
    "*   Comparación, discusión y conclusiones\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnlmuDJC2Kae"
   },
   "source": [
    "# Importación de librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "055OxKW_1Ycp",
    "outputId": "75b9958d-f9d1-4b1d-94fd-64b605580d7d"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import os\n",
    "#import chardet\n",
    "from collections import Counter\n",
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive\")\n",
    "#path = '/content/drive/My Drive/MG Data Science/MLA/Tarea 3/'\n",
    "\n",
    "### Ahora en Keras\n",
    "\n",
    "from builtins import range, input\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dot, Add, Flatten\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6IC-4xo2MJp"
   },
   "source": [
    "# Procesamiento de datos y exploración descriptiva:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPKJr_wHEi7P"
   },
   "source": [
    "**Resumen de los campos:**\n",
    "\n",
    "**display_id**: Identificación del contexto o situación en la que un usuario clickeó un aviso.\n",
    "\n",
    "**ad_id**: Identificación del aviso publicitario.\n",
    "\n",
    "**clicked**: Columna binaria que simboliza si el usiario clickeó o no.\n",
    "\n",
    "**document_id**: Identificación de una página web\n",
    "\n",
    "**category_id**: Identificación del tipo de contenido de un ad.\n",
    "\n",
    "**confidence_level**: Confianza que tiene la empresa proveedora entre una relación.\n",
    "\n",
    "**campaing_id**: Identificación de la campaña a la cual pertenece un ad.\n",
    "\n",
    "**advertiser_id**: Identificación de la empresa a la cual pertenece un ad.\n",
    "\n",
    "**uuid**: Identificación de un usuario.\n",
    "\n",
    "**timestamp**: Fecha de ejecución.\n",
    "\n",
    "**platform**: Si es un (1) desktop, (2) celular o (3) laptop.\n",
    "\n",
    "**geolocation**: País>Estado>DMA\n",
    "\n",
    "**topic_id**: Identificación del tópico del aviso.\n",
    "\n",
    "**traffic_source**: (1) interno, (2) búsqueda o (3) social.\n",
    "\n",
    "**entity_id**: Identificación de una persona, organización o lugar.\n",
    "\n",
    "**publisher_id**: Identificación del editorial.\n",
    "\n",
    "**publish_time**: Momento de una publicación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2sBSgqlmVEq"
   },
   "source": [
    "###Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "DFkvl1E8mUeb",
    "outputId": "c57f11e0-b0f3-4232-c962-c50704879204"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>display_id</th>\n",
       "      <th>ad_id</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>42337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>139684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>144739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>156824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>279295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87141726</th>\n",
       "      <td>16874592</td>\n",
       "      <td>186600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87141727</th>\n",
       "      <td>16874593</td>\n",
       "      <td>151498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87141728</th>\n",
       "      <td>16874593</td>\n",
       "      <td>282350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87141729</th>\n",
       "      <td>16874593</td>\n",
       "      <td>521828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87141730</th>\n",
       "      <td>16874593</td>\n",
       "      <td>522693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87141731 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          display_id   ad_id  clicked\n",
       "0                  1   42337        0\n",
       "1                  1  139684        0\n",
       "2                  1  144739        1\n",
       "3                  1  156824        0\n",
       "4                  1  279295        0\n",
       "...              ...     ...      ...\n",
       "87141726    16874592  186600        0\n",
       "87141727    16874593  151498        1\n",
       "87141728    16874593  282350        0\n",
       "87141729    16874593  521828        0\n",
       "87141730    16874593  522693        0\n",
       "\n",
       "[87141731 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clicks_train = pd.read_csv('clicks_train.csv')\n",
    "df_clicks_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1d2YDZ0BoLj"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "MgUoKgL8BY5u",
    "outputId": "96347a9b-47d2-45c3-b98e-710ee1dfbb5d"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'clicks_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16740\\2257213637.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_clicks_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clicks_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_clicks_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clicks_test.csv'"
     ]
    }
   ],
   "source": [
    "df_clicks_test = pd.read_csv('clicks_test.csv')\n",
    "df_clicks_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WczaeuHlnMXr"
   },
   "source": [
    "## Documents Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8OoqIvk8A8Uw",
    "outputId": "1d396568-b2f3-4fd2-d5f3-51cc8a7ba582"
   },
   "outputs": [],
   "source": [
    "df_documents_categories = pd.read_csv('Tarea3/documents_categories.csv')\n",
    "df_documents_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAJcdfxJ2V-D"
   },
   "source": [
    "## Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4pCasMo21qnE",
    "outputId": "5ec2aa92-837e-4951-8eb8-894c1098e240"
   },
   "outputs": [],
   "source": [
    "df_sample_sub = pd.read_csv('Tarea3/sample_submission.csv')\n",
    "df_sample_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQph7fDS2fvp"
   },
   "source": [
    "## Promoted Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "tSSGLveP2juX",
    "outputId": "57f6050f-086c-465f-f576-800701affdce"
   },
   "outputs": [],
   "source": [
    "df_promoted_cont = pd.read_csv('Tarea3/promoted_content.csv')\n",
    "df_promoted_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qroLGTN20So"
   },
   "source": [
    "## Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "EeOfnqPi23Wx",
    "outputId": "b4262361-a32c-4663-819d-7ae589d7ca2f"
   },
   "outputs": [],
   "source": [
    "df_events = pd.read_csv('Tarea3/events.csv')\n",
    "df_events = df_events[['uuid','display_id','document_id','timestamp','platform','geo_location']]\n",
    "df_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9N3H-reaReLO"
   },
   "source": [
    "Hay usuarios que tienen más de un display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "XVp4EVYK-snB",
    "outputId": "6287acfd-5c3d-401f-e08e-385070df60c2"
   },
   "outputs": [],
   "source": [
    "df_documents_topics = pd.read_csv('Tarea3/documents_topics.csv')\n",
    "df_documents_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ek3YX9i--7ga"
   },
   "source": [
    "## Page Views Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Eu5QztW3AVH"
   },
   "source": [
    "## Document Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zKbSPSN-sjp"
   },
   "outputs": [],
   "source": [
    "df_page_view_sample = pd.read_csv('Tarea3/page_views_sample.csv')\n",
    "df_page_view_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkbrFEVaAwhm"
   },
   "source": [
    "## Document Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "tI4qWNsSARpB",
    "outputId": "dd96bfd8-0282-412b-ea84-9b0c8bb7838f"
   },
   "outputs": [],
   "source": [
    "df_documents_entities = pd.read_csv('Tarea3/documents_entities.csv')\n",
    "df_documents_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5m-OTJlAzBX"
   },
   "source": [
    "## Documents Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8DhqSLtsA17s",
    "outputId": "749c08a9-5e92-45f9-8068-33c7db4865e9"
   },
   "outputs": [],
   "source": [
    "df_documents_meta = pd.read_csv('Tarea3/documents_meta.csv')\n",
    "df_documents_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUg4T_LscccN"
   },
   "source": [
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VO-JDTRncbeB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF6URG6NItXP"
   },
   "source": [
    "# Exploración descriptiva:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC5QudJP-_86"
   },
   "source": [
    "Dentro del dataset, la base que se utilizó para creación del modelo predictivo con factorización de matrices es \"click_train.csv\". Esta data cuenta con 87.141.731 filas y 3 columnas.\n",
    "\n",
    "La cantidad de ads máxima que un usuario clickeó dentro de la base es 12, mientras que el mínimo es 2, siendo 4 el número más común de ads clickeados, seguido de 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "VCEjCWjEIwrj",
    "outputId": "e31d80d5-732c-4522-9f7f-d16aed3f2dfa"
   },
   "outputs": [],
   "source": [
    "ad_in_display = df_clicks_train.groupby('display_id')['ad_id'].count().value_counts()\n",
    "sns.barplot(ad_in_display.index, ad_in_display.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdVLQ7oUGync"
   },
   "source": [
    "También es posible extraer información valiosa de otras bases de datos contenidas en este desafío, por ejemplo, es posible graficar la cantidad de veces que aparecen publicados los ads en las distintas plataformas. Un 61,7% de los ads aparece hasta 10 veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "6brULpUJUtAn",
    "outputId": "2055252e-719c-4851-98d7-f7ce36a796d3"
   },
   "outputs": [],
   "source": [
    "ad_usage_train = df_clicks_train.groupby('ad_id')['ad_id'].count()\n",
    "\n",
    "for i in [2, 10, 50, 100, 1000]:\n",
    "    print('Ads that appear less than {} times: {}%'.format(i, round((ad_usage_train < i).mean() * 100, 2)))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(ad_usage_train.values, bins=50, log=True)\n",
    "plt.xlabel('Number of times ad appeared', fontsize=12)\n",
    "plt.ylabel('log(Count of displays with ad)', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoax7MeIR10M"
   },
   "source": [
    "# Modelo de Factorización de Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, queremos dejar como valores categoricos a las dos columnas las cuales vamos a utilizar para el entrenamiento, que en este caso son las de display_id y ad_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2brqCQ-7Ud0f"
   },
   "outputs": [],
   "source": [
    "df_clicks_train.display_id = df_clicks_train.display_id.astype('category').cat.codes.values\n",
    "df_clicks_train.ad_id = df_clicks_train.ad_id.astype('category').cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues, ya que esta dataframe contiene muchisimos datos (no tenemos la capacidad computacional para poder procesar tantos datos) necesitamos hacerle una reduccion, y para hacer esto ocupamos la libreria Counter para buscar los usuarios y anuncios que mas se repiten en la data. Al hacer esto, podemos reducir los datos sin perder tanta precision ya que estamos utilizando los datos mas comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "m = 8000\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucount = Counter(df_clicks_train['display_id'])\n",
    "mcount = Counter(df_clicks_train['ad_id'])\n",
    "\n",
    "uid = [u for u, c in ucount.most_common(n)]\n",
    "mid = [u for u, c in mcount.most_common(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df_clicks_train[df_clicks_train['display_id'].isin(uid) & df_clicks_train['ad_id'].isin(mid)]\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos la misma transformacion de datos para la nueva dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.display_id = newdf.display_id.astype('category').cat.codes.values\n",
    "newdf.ad_id = newdf.ad_id.astype('category').cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(newdf, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_ads = 10000 , 8000\n",
    "n_latent_factors = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la fase de entrenamiento, al estar tratando con una factorizacion de matrices, debemos utilizar el producto punto entre las matrices de anuncios y personas, por lo que para eso hacemos un embedding y despues un flatten. Al final, podemos hacerle el producto punto a las matrices flatten de usuarios y anuncios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_input = keras.layers.Input(shape=[1],name='Item')\n",
    "ad_embedding = keras.layers.Embedding(n_ads + 1, n_latent_factors, name='Ad-Embedding')(ad_input)\n",
    "ad_vec = keras.layers.Flatten(name='FlattenAds')(ad_embedding)\n",
    "user_input = keras.layers.Input(shape=[1],name='User')\n",
    "user_vec = keras.layers.Flatten(name='FlattenUsers')(keras.layers.Embedding(n_users + 1, n_latent_factors,name='User-Embedding')(user_input))\n",
    "prod = keras.layers.dot([ad_vec, user_vec], axes=1,name='DotProduct')\n",
    "print(prod)\n",
    "model = keras.Model([user_input, ad_input], prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([train.display_id, train.ad_id], train.clicked, epochs=25, verbose=0, \n",
    "                    validation_data = ([test.display_id, test.ad_id], test.clicked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui podemos observar el tema de overfitting que existe dentro de los modelos de factorizacion de matrices y sus derivados. Dado que estos modelos no son generalizables, estos se ajustan mucho a los valores actuales y lo que tiende a pasar es que para los datos de entrenamiento existe perdida pero cuando entramos a ver los datos de validacion estos tienden a subir, o a bajar y subir. En este caso, los datos de validacion suben mientras que los de entrenamiento bajan, y despues ambos bajan con la misma pendiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(history.history['loss']).plot(logy=True, color='b')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(history.history['val_loss']).plot(logy=True, color='orange')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate((test.display_id, test.ad_id), test.clicked, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones Factorizacion de Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finalmente, como queremos hacer predicciones, vamos a predecir los primeros cinco anuncios que se le podrian dar a una persona dado que ha visto anuncios anteriormente.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_embedding_learnt = model.get_layer(name='Ad-Embedding').get_weights()[0]\n",
    "pd.DataFrame(ad_embedding_learnt).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedding_learnt = model.get_layer(name='User-Embedding').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, number_of_ads=5):\n",
    "    ads = user_embedding_learnt[user_id]@ad_embedding_learnt.T\n",
    "    mids = np.argpartition(ads, -number_of_ads)[-number_of_ads:]\n",
    "    return mids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estos son los 5 ad_id que le estariamos recomendando a nuestro  primer usuario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(user_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Factorizacion de Maquinas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos creando una clase la cual va a contener todas las columnas necesarias para ejecutar el modelo, ademas de eso creamos las caracteristicas necesarias para el entrenamiento posterior, como la semilla, las epocas, y el valor de regularizacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    category_col = ['display_id', 'ad_id', 'uuid', 'document_id',\n",
    "       'campaign_id', 'advertiser_id']\n",
    "    num_col = []\n",
    "    target_col = ['clicked']\n",
    "    \n",
    "    seed=2021\n",
    "    epochs=5\n",
    "    batch_size=128\n",
    "    seed=17\n",
    "    embedding_dim=8\n",
    "    lr=1e-4\n",
    "    \n",
    "config=Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df_clicks_train\n",
    "item_df = df_promoted_cont\n",
    "user_df = df_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un drop a las columnas de localizacion dado a que no nos ofrece informacion inmediata para procesarla en el modelo, el document id por temas de incongruencias con la dataframe de items, el cual ya trae un document id. El timestamp, el cual solo trae informacion de tiempo y no es tan necesario procesarla en este modelo. Y por ultimo, el platform, el cual trae problemas al hacer preprocesamiento, dado que trae dentro de el datos vacios los cuales no fuimos capaces de rellenar, dando problemas al hacer la transformacion de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.drop(columns=['geo_location', 'document_id', 'timestamp', 'platform'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo, juntamos los datos para poder procesarlos\n",
    "\n",
    "\n",
    "Es de notar para este ejercicio las pocas columnas que estamos utilizando, esto es dado a la gran cantidad de datos que ofrece el ejercicio, ya que, para los datos de documentacion, al juntarlos con los datos principales llegamos a una cantidad de datos de aproximadamente 1700 mil millones los cuales consumen mucha memoria y para nosotros no es posible juntarlos, por lo que por temas de memoria y simplicidad decidimos sacarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(data_df, item_df, user_df):\n",
    "    tmp = pd.merge(data_df, user_df, on='display_id', how='inner')\n",
    "    tmp = pd.merge(tmp, item_df, on='ad_id', how='inner')\n",
    "    tmp = tmp\n",
    "    return tmp\n",
    "\n",
    "df = merge_df(data_df, item_df, user_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya despues de haber juntado los datos, procedemos a crear dos clases, una la cual construye las funciones para el preprocesamiento, y otra la cual hace el preprocesamiento. En la primera, utilizamos la clase creada al comienzo para pasarle una funcion Pipeline, la cual nos da el orden en el cual se va a ejecutar el modelo, llenando los datos numericos con un promedio de los demas datos, y datos NaN para las columnas categoricas. Despues, se le hace una transformacion con la funcion ColumnTransformer a los datos numericos y categoricos. Como es de notar en nuestro caso, nostros no vamos a utilizar datos numericos, por lo cual las funciones para los datos numericos no son de utilidad para nosotros en este momento. \n",
    "\n",
    "Despues, la funcion de preprocesamiento hace la reduccion de dimensiones al pasarle como argumento cuantas veces aparece un anuncio dentro del dataframe y por ultimo utilizando la funcion anterior genera el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def build_preprocessor(config): \n",
    "    category_col = config.category_col\n",
    "    num_col = config.num_col\n",
    "    \n",
    "    num_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        ('std', (StandardScaler())),])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value='NAN')),\n",
    "        ('oe', (OrdinalEncoder())),\n",
    "        ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer, num_col),\n",
    "            ('cat', categorical_transformer, category_col),\n",
    "        ],\n",
    "        remainder=\"drop\")\n",
    "    return preprocessor\n",
    "    \n",
    "def preprocess(df, config):\n",
    "    \n",
    "    category_col = config.category_col\n",
    "    num_col = config.num_col\n",
    "    target_col = config.target_col\n",
    "    \n",
    "    print(target_col)\n",
    "    \n",
    "    print(df.shape)\n",
    "    ad_cnt = df.groupby('ad_id').size()\n",
    "    use_ad = list(ad_cnt[ad_cnt > 210000].index)\n",
    "    df = df[df['ad_id'].isin(use_ad)]\n",
    "    print(df.shape)\n",
    "\n",
    "    # Extract release year.\n",
    "    #df[\"year\"] = df['release_date'].apply(lambda x: str(x).split('-')[-1]) \n",
    "\n",
    "    # Create a label column for binary classification.\n",
    "    #df.insert(df.shape[1],target_col,df['clicked'],True)\n",
    "    #df[target_col] = df['clicked'] >= 4.0\n",
    "    #df[target_col] = df[target_col].astype(int)\n",
    "\n",
    "    # Build pipeline\n",
    "    pp = build_preprocessor(config)\n",
    "    pp.fit(df)\n",
    "    return df, pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, pp = preprocess(df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.transform(df).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos el entrenamiento haciendo la separacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "tra_df, val_df = train_test_split(df, test_size=0.2, stratify=df['ad_id'], random_state=config.seed)\n",
    "print(tra_df.shape)\n",
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la construccion del modelo, hay que tomar en cuenta la adicion de features al modelo de la factorizacion de matrices lo cual nos deja con muchos multiplicaciones por hacer. Es por esto que despues de hacerle el embedding a todas las columnas, las cuales nos ayudan a procesar datos grandes, le hacemos un flattening para despues proceder a hacer el producto punto y por ultimo la adicion de todas esas multiplicaciones para llegar al resultado del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, add, Activation, dot\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2 as l2_reg\n",
    "import itertools\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_model(category_num, category_cols, num_cols, K=20, solver='adam', l2=0, l2_fm=1e-3):\n",
    "\n",
    "    # Numerical features\n",
    "    num_inputs = [Input(shape=(1,), name=col,) for col in num_cols]\n",
    "    # Categorical features\n",
    "    cat_inputs = [Input(shape=(1,), name=col,) for col in category_cols]\n",
    "\n",
    "    inputs = num_inputs + cat_inputs\n",
    "\n",
    "    flatten_layers=[]\n",
    "    # Numerical featrue embedding\n",
    "    for enc_inp, col in zip(num_inputs, num_cols):\n",
    "        # num featrue dence\n",
    "        x = Dense(K, name = f'embed_{col}',kernel_regularizer=l2_reg(l2_fm))(enc_inp)\n",
    "        flatten_layers.append(x)\n",
    "\n",
    "    # Category feature embedding\n",
    "    for enc_inp, col in zip(cat_inputs, category_cols):\n",
    "        num_c = category_num[col]\n",
    "        embed_c = Embedding(input_dim=num_c,\n",
    "                            output_dim=K,\n",
    "                            input_length=1,\n",
    "                            name=f'embed_{col}',\n",
    "                            embeddings_regularizer=l2_reg(l2_fm))(enc_inp)\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "        flatten_layers.append(flatten_c)\n",
    "                \n",
    "    # Feature interaction term\n",
    "    fm_layers = []\n",
    "    for emb1,emb2 in itertools.combinations(flatten_layers, 2):\n",
    "        dot_layer = dot([emb1,emb2], axes=1)\n",
    "        fm_layers.append(dot_layer)\n",
    "    #print(fm_layers)\n",
    "        \n",
    "\n",
    "    # Linear term\n",
    "    for enc_inp,col in zip(cat_inputs, category_cols):\n",
    "        # embedding\n",
    "        num_c = category_num[col]\n",
    "        embed_c = Embedding(input_dim=num_c,\n",
    "                            output_dim=1,\n",
    "                            input_length=1,\n",
    "                            name=f'linear_{col}',\n",
    "                            embeddings_regularizer=l2_reg(l2_fm))(enc_inp)\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "        fm_layers.append(flatten_c)\n",
    "                \n",
    "    for enc_inp, col in zip(num_inputs, num_cols):\n",
    "        x = Dense(1, name = f'linear_{col}',kernel_regularizer=l2_reg(l2_fm))(enc_inp)\n",
    "        fm_layers.append(x)\n",
    "\n",
    "    # Add all terms\n",
    "    flatten = add(fm_layers)\n",
    "    outputs = Activation('sigmoid',name='outputs')(flatten)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "                optimizer=solver,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics='accuracy'\n",
    "              )\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_num = {col: df[col].nunique() for col in config.category_col}\n",
    "model = build_model(category_num, config.category_col, config.num_col, K=config.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ver los resultados, se nota que los valores de perdida en especial en los datos de validacion son minusculos, y esto se debe al gran problema de overfitting que tienen las factorizaciones de matrices y maquinas. Para esto ultimo, utilizamos el regularizador l2 el cual nos ayuda a castigar los pesos asociados en el modelo, aproximandolos a 0 pero nunca dejandolos en 0. Esto disminuye la influencia que tienen los pesos por sobre los nodos y nos sirve para quitar el overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=2, verbose=0,)]\n",
    "\n",
    "feature_num = len(config.category_col + config.num_col)\n",
    "tra_inputs = [pp.transform(tra_df)[:, i] for i in range(feature_num)]\n",
    "val_inputs = [pp.transform(val_df)[:, i] for i in range(feature_num)]\n",
    "\n",
    "history = model.fit(\n",
    "          #x=pp.transform(tra_df).reshape(len(tra_df), feature_num, 1),\n",
    "          x=tra_inputs,\n",
    "          y=tra_df[config.target_col],\n",
    "          epochs=config.epochs,\n",
    "          batch_size=config.batch_size,\n",
    "          validation_data=(val_inputs,\n",
    "                           val_df[config.target_col]),\n",
    "          callbacks=cb\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los graficos se ve mas claro el tema del potencial overfitting que puede existir facilmente en estos modelos de FM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "    plt.show()    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo, para poder hacer predicciones, tomamos a un usuario y predecimos si el le fuera a hacer click a un anuncio o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user='a8adbb70abf1a5' #user_id\n",
    "user_df = val_df.reset_index(drop=True).query('uuid==@user')\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = [pp.transform(val_df)[user_df.index, i] for i in range(feature_num)]\n",
    "user_df['pred'] = model.predict(user_inputs)\n",
    "user_df = user_df.sort_values('pred', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df[['ad_id','clicked','pred']].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conclusión, se puede ver que existen algunas diferencias entre la factorización de matrices y la factorización de maquinas, por lo cual existen ventajas y desventajas entre ellas: Una desventaja de la factorización de matrices, viene siendo que es no es generalizable, son especificamente construidos para un problema dado. Los algoritmos de aprendizaje e implementación son hechos a medida para modelos individuales. Una ventaja es que podemos estimar o calcular interacciones entre dos o mas variables incluso si la interaccion no ha sido observada, o sea, podemos pasarle usuarios y peliculas, y que este nos arroje un valor que ese usuario le fuera a dar a una pelicula, si es que la hubiese visto.\n",
    "\n",
    "En el caso de la factorización de máquinas, sabemos que es un modelo derivado que utiliza la factorización de matrices, pero tiene la ventaja de que puede utilizar la regresión polinomial, por lo que esto nos ayuda a poder tomar mas variables,  llegando a un modelo mas generalizable. Una desventaja que tienen ambas es que requieren mas cálculo que otros modelos similares, y en especial el de factorización de máquinas. Mientras más columnas con datos le agreguemos, más parametros tendrá el modelo al final y más tiempo se consumirá para poder entrenar el modelo. Por lo que, el modelo de factorización de máquinas dependiendo de la capacidad computacional, podria no ser bueno dado a la poca cantidad de datos que puedes utilizar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
